## ğŸ“Š Comparative Analysis of Large Language Models in Retrieval-Augmented Generation (RAG) Systems

A comprehensive performance study comparing open-source LLMs for RAG applications, conducted within 15GB GPU memory constraints.

##  Technical Stack

- **LLMs**: HuggingFace Transformers
- **Vector Database**: ChromaDB
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2
- **Document Processing**: PyPDF, LangChain
- **Environment**: Kaggle (15GB GPU memory)

## Acknowledgments

- HuggingFace for providing the model implementations
- LangChain for the RAG pipeline framework
- ChromaDB for the vector database
- Kaggle for providing the computational resources

Results
SetupÂ Performance
Model	Model Load (s)
LLaMAÂ 2Â 7B	7.04,
Falcon 7B	59.48,
MistralÂ 7B	165.29

Answer Generation Performance
Model	Average Answer Time (s)	Quality Rating
LLaMA 2Â 7B	3.08	â­â­â­â­,
Falcon 7B	8.92	â­â­â­â­,
Mistral 7B	23.78	â­â­â­â­â­


Sample Q&A Results
Model	Answer Time	Response Quality
LLaMAÂ 2 7B	2.21s	Concise, accurate definition,
FalconÂ 7B	10.06s	Brief, factualÂ response,
Mistral 7B	20.90s	Detailed, comprehensive explanation
